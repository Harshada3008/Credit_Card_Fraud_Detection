{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc53c7-cece-4a60-89e1-7aaf83f175bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CreditCardMLPipeline:\n",
    "    def __init__(self, experiment_name=\"credit_card_fraud_detection\"):\n",
    "        \"\"\"Initialize the ML pipeline with MLflow experiment\"\"\"\n",
    "        self.experiment_name = experiment_name\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Set up MLflow\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        print(f\"MLflow experiment '{experiment_name}' is ready!\")\n",
    "    \n",
    "    def load_dataset(self, dataset_path=None):\n",
    "        \"\"\"Load credit card dataset automatically\"\"\"\n",
    "        if dataset_path:\n",
    "            # Load custom dataset\n",
    "            self.data = pd.read_csv(dataset_path)\n",
    "            print(f\"Loaded custom dataset with shape: {self.data.shape}\")\n",
    "        else:\n",
    "            # Generate synthetic credit card fraud dataset\n",
    "            print(\"Generating synthetic credit card dataset...\")\n",
    "            np.random.seed(42)\n",
    "            n_samples = 10000\n",
    "            \n",
    "            # Generate features\n",
    "            data = {\n",
    "                'V1': np.random.normal(0, 1, n_samples),\n",
    "                'V2': np.random.normal(0, 1, n_samples),\n",
    "                'V3': np.random.normal(0, 1, n_samples),\n",
    "                'V4': np.random.normal(0, 1, n_samples),\n",
    "                'V5': np.random.normal(0, 1, n_samples),\n",
    "                'Amount': np.random.exponential(50, n_samples),\n",
    "                'Time': np.random.uniform(0, 172800, n_samples)  # 48 hours in seconds\n",
    "            }\n",
    "            \n",
    "            # Create fraud labels (imbalanced - 2% fraud)\n",
    "            fraud_indices = np.random.choice(n_samples, int(0.02 * n_samples), replace=False)\n",
    "            data['Class'] = np.zeros(n_samples)\n",
    "            data['Class'][fraud_indices] = 1\n",
    "            \n",
    "            # Make fraudulent transactions different\n",
    "            for idx in fraud_indices:\n",
    "                data['V1'][idx] += np.random.normal(2, 0.5)\n",
    "                data['V2'][idx] += np.random.normal(-1.5, 0.5)\n",
    "                data['Amount'][idx] *= np.random.uniform(0.1, 0.3)\n",
    "            \n",
    "            self.data = pd.DataFrame(data)\n",
    "            print(f\"Generated synthetic dataset with shape: {self.data.shape}\")\n",
    "            print(f\"Fraud cases: {self.data['Class'].sum()} ({self.data['Class'].mean()*100:.2f}%)\")\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"Preprocess the dataset\"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "        \n",
    "        # Separate features and target\n",
    "        X = self.data.drop('Class', axis=1)\n",
    "        y = self.data['Class']\n",
    "        \n",
    "        # Handle any categorical variables if present\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'object':\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col])\n",
    "        \n",
    "        # Split the data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        print(f\"Training set shape: {self.X_train.shape}\")\n",
    "        print(f\"Test set shape: {self.X_test.shape}\")\n",
    "        \n",
    "        return self.X_train_scaled, self.X_test_scaled, self.y_train, self.y_test\n",
    "    \n",
    "    def initialize_models(self):\n",
    "        \"\"\"Initialize 5 different ML models\"\"\"\n",
    "        self.models = {\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "            'SVM': SVC(probability=True, random_state=42),\n",
    "            'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=1000)\n",
    "        }\n",
    "        print(f\"Initialized {len(self.models)} models for training\")\n",
    "        return self.models\n",
    "    \n",
    "    def train_and_evaluate_models(self):\n",
    "        \"\"\"Train all models and log results to MLflow\"\"\"\n",
    "        print(\"Training and evaluating models...\")\n",
    "        \n",
    "        for model_name, model in self.models.items():\n",
    "            print(f\"\\nTraining {model_name}...\")\n",
    "            \n",
    "            with mlflow.start_run(run_name=model_name):\n",
    "                # Train model\n",
    "                model.fit(self.X_train_scaled, self.y_train)\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred = model.predict(self.X_test_scaled)\n",
    "                y_pred_proba = model.predict_proba(self.X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(self.y_test, y_pred)\n",
    "                precision = precision_score(self.y_test, y_pred, zero_division=0)\n",
    "                recall = recall_score(self.y_test, y_pred, zero_division=0)\n",
    "                f1 = f1_score(self.y_test, y_pred, zero_division=0)\n",
    "                \n",
    "                # AUC score if probability predictions available\n",
    "                auc = roc_auc_score(self.y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "                \n",
    "                # Cross-validation score\n",
    "                cv_scores = cross_val_score(model, self.X_train_scaled, self.y_train, cv=5, scoring='f1')\n",
    "                cv_mean = cv_scores.mean()\n",
    "                cv_std = cv_scores.std()\n",
    "                \n",
    "                # Store results\n",
    "                self.results[model_name] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'auc': auc,\n",
    "                    'cv_mean': cv_mean,\n",
    "                    'cv_std': cv_std,\n",
    "                    'model': model\n",
    "                }\n",
    "                \n",
    "                # Log parameters to MLflow\n",
    "                if hasattr(model, 'get_params'):\n",
    "                    params = model.get_params()\n",
    "                    for param, value in params.items():\n",
    "                        mlflow.log_param(param, value)\n",
    "                \n",
    "                # Log metrics to MLflow\n",
    "                mlflow.log_metric(\"accuracy\", accuracy)\n",
    "                mlflow.log_metric(\"precision\", precision)\n",
    "                mlflow.log_metric(\"recall\", recall)\n",
    "                mlflow.log_metric(\"f1_score\", f1)\n",
    "                if auc is not None:\n",
    "                    mlflow.log_metric(\"auc\", auc)\n",
    "                mlflow.log_metric(\"cv_mean_f1\", cv_mean)\n",
    "                mlflow.log_metric(\"cv_std_f1\", cv_std)\n",
    "                \n",
    "                # Log model\n",
    "                mlflow.sklearn.log_model(model, \"model\")\n",
    "                \n",
    "                auc_str = f\"{auc:.4f}\" if auc is not None else \"N/A\"\n",
    "                print(f\"{model_name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}, AUC: {auc_str}\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def display_results_summary(self):\n",
    "        \"\"\"Display comprehensive results summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame.from_dict(self.results, orient='index')\n",
    "        results_df = results_df.round(4)\n",
    "        \n",
    "        # Sort by F1 score (important for imbalanced dataset)\n",
    "        results_df_sorted = results_df.sort_values('f1', ascending=False)\n",
    "        \n",
    "        print(results_df_sorted[['accuracy', 'precision', 'recall', 'f1', 'auc', 'cv_mean']].to_string())\n",
    "        \n",
    "        # Best model\n",
    "        best_model_name = results_df_sorted.index[0]\n",
    "        best_model = self.results[best_model_name]['model']\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "        print(f\"   F1 Score: {results_df_sorted.loc[best_model_name, 'f1']:.4f}\")\n",
    "        print(f\"   Accuracy: {results_df_sorted.loc[best_model_name, 'accuracy']:.4f}\")\n",
    "        print(f\"   AUC: {results_df_sorted.loc[best_model_name, 'auc']:.4f}\")\n",
    "        \n",
    "        # Classification report for best model\n",
    "        y_pred_best = best_model.predict(self.X_test_scaled)\n",
    "        print(f\"\\nDetailed Classification Report for {best_model_name}:\")\n",
    "        print(classification_report(self.y_test, y_pred_best))\n",
    "        \n",
    "        return results_df_sorted, best_model_name\n",
    "    \n",
    "    def plot_model_comparison(self):\n",
    "        \"\"\"Create visualization comparing model performance\"\"\"\n",
    "        # Prepare data for plotting\n",
    "        model_names = list(self.results.keys())\n",
    "        metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, metric in enumerate(metrics):\n",
    "            ax = axes[i//2, i%2]\n",
    "            values = [self.results[model][metric] for model in model_names]\n",
    "            \n",
    "            bars = ax.bar(model_names, values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum'])\n",
    "            ax.set_title(f'{metric.capitalize()} Comparison', fontweight='bold')\n",
    "            ax.set_ylabel(metric.capitalize())\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, value in zip(bars, values):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Log plot to MLflow\n",
    "        with mlflow.start_run(run_name=\"model_comparison\"):\n",
    "            plt.savefig(\"model_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "            mlflow.log_artifact(\"model_comparison.png\")\n",
    "    \n",
    "    def run_complete_pipeline(self, dataset_path=None):\n",
    "        \"\"\"Run the complete ML pipeline\"\"\"\n",
    "        print(\"üöÄ Starting Credit Card Fraud Detection ML Pipeline\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Load dataset\n",
    "        self.load_dataset(dataset_path)\n",
    "        \n",
    "        # Step 2: Preprocess data\n",
    "        self.preprocess_data()\n",
    "        \n",
    "        # Step 3: Initialize models\n",
    "        self.initialize_models()\n",
    "        \n",
    "        # Step 4: Train and evaluate models\n",
    "        self.train_and_evaluate_models()\n",
    "        \n",
    "        # Step 5: Display results\n",
    "        results_df, best_model = self.display_results_summary()\n",
    "        \n",
    "        # Step 6: Create visualizations\n",
    "        self.plot_model_comparison()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Pipeline completed successfully!\")\n",
    "        print(f\"üî¨ Check MLflow UI with: mlflow ui\")\n",
    "        print(f\"üìä Experiment: {self.experiment_name}\")\n",
    "        \n",
    "        return results_df, best_model\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Create and run the pipeline\n",
    "    pipeline = CreditCardMLPipeline()\n",
    "    \n",
    "    # Option 1: Run with synthetic data\n",
    "    results, best_model_name = pipeline.run_complete_pipeline()\n",
    "    \n",
    "    # Option 2: Run with your own dataset (uncomment below)\n",
    "    # results, best_model_name = pipeline.run_complete_pipeline(\"path/to/your/creditcard.csv\")\n",
    "    \n",
    "    print(f\"\\nüéØ Best performing model: {best_model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
